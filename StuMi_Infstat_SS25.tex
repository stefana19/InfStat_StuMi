\documentclass[10pt]{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[ngerman]{babel} % Sprach - Package

% Mathematische Formatierung
\usepackage{amssymb, amsmath, amsfonts} % Mathematische Formeln
\boldmath
\newcommand{\FZV}{X_1, \ldots, X_n} % Folge von Zufallsvariablen
\newcommand{\IR}{\mathbb{R}} % reeler Wertebreich
\newcommand{\EW}{\mathbb{E}} % Erwartungswert
\newcommand{\KW}{\overset{p} \longrightarrow} %Konvergenz in Wahrscheinlichkeit
\newcommand{\KV}{\overset{\sim} \longrightarrow} %Konvergenz in Verteilung
\newcommand{\eqname}[1]{\tag*{#1}}% Tag equation with name
\newcommand{\xt}{x \mid \theta} %x bedingt auf theta



% Margin-Anpassung
\usepackage[inner = 2.5cm, outer = 2.5cm , top = 4cm, bottom = 4cm, includeheadfoot]{geometry}
\addtolength{\textheight}{+2.5in}
\addtolength{\topmargin}{-1.3in}

% Formatierung und Visuelles
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage[most]{tcolorbox}
\usepackage[framemethod]{mdframed}
\usepackage{enumerate} % Fuer nummerierte Listen
\usepackage{enumitem} % Eigene Nummerierung erstellen
\newmdenv[
topline = false,
rightline = false,
bottomline = false, 
leftline = true,
linecolor = brown, 
linewidth = 3pt,
backgroundcolor = brown!5,
frametitle =
]{Beispiel}
\newenvironment{BSP}[1][]
{\begin{Beispiel}[frametitle=#1]}{\end{Beispiel}}
\newmdenv[
topline = false,
rightline = false,
bottomline = false, 
leftline = true,
linecolor = lime, 
linewidth = 3pt,
backgroundcolor = lime!5,
frametitle =
]{Beweis}
\newenvironment{BWS}[1][]
{\begin{Beweis}[frametitle=#1]}{\end{Beweis}}
\newtcolorbox{Definition}[1][Heading]{ %Farbbox Definition 
title = \textbf{#1},
colback = cyan!15, % Hintergrund Farbbox
colframe = cyan!65, %Randfarbe Box 
coltitle = black, %Titel Box
}
\usepackage{setspace}  % Fuer line spacing (T13.04.2025)
\onehalfspacing	% (T13.04.2025)






\title{Inferenzstatistik SoSe 2025}
\author{Stefan Aichmann}
\date{March 2025}

\begin{document}
	
	\maketitle \newpage
	\tableofcontents \newpage
	
	\part{Schätzen von Parametern}
	
	\section{Parametrische Modelle}
	Man betrachte ein Experiment mit $\FZV$ Zufallsvariablen und trifft die Annahme, dass $\FZV$ i.i.d. Kopien von einer Zufallsvariable $X$ darstellen und einer Verteilung mit der Form
	\begin{equation*}
	 X \sim \; \textbf{F}(\theta_0)
	\end{equation*}
	
	\noindent folgen, wobei $\textbf{F}(\theta_0)$ die Verteilung von $X$ darstellt, welche durch den Paratemer $\theta_0$ beschrieben wird. Normalerweise ist $\theta_0$ nicht bekannt, weswegen  man versucht, den Parameter zu schätzen. Glücklicherweise ist der mögliche Wertebereich des unbekannten Parameters oftmals bekannt und man schreibt
	\begin{equation*}
		\theta \in \Theta \subseteq \IR^k.
	\end{equation*}
	
	\noindent In diesem Fall spricht man von einem $\textbf{parametrischen Modell}$.\\
	Folgende Dinge sind zu beachten: 
	
	\begin{enumerate}
      \item $\theta_0$ ist unbekannt, aber man weiß, dass $\theta_0 \in \Theta$ liegt. 
      \item  Jeder mögliche Wert $\theta \in \Theta$ des unbekannten Parameters entspricht genau einer Verteilung von $X$ (und $\FZV$), die mit $\mathbb{P}_\theta$ bzw. $\mathbb{E}_\theta$ bezeichnet wird.
	\end{enumerate}
	
	
	\begin{Definition}[Definition 1.1 (Schätzer)]
		Ein Schätzer $\hat{\theta}$ für $\theta_0$ ist eine Funktion 
		\begin{equation*}
				\hat{\theta} = \hat{\theta} (\FZV)
		\end{equation*}
	 mit Werten in $\IR ^k$, die nur von den Beobachtungen (und weiteren bekannten Größen wie zum Beispiel $n$), aber nicht von $\theta_0$ abhängt.
	 Es gilt:
	 \begin{equation*}
	 	\text{dim}(\hat{\theta}) = \text{dim}(\theta_0) = k.
	 \end{equation*}
	\end{Definition}
	 
	 \begin{BSP}[Beispiel 1.2 (Mittelwertschätzer)]
	 	Ein bekanntes Beispiel eines Schätzers ist der Mittelwertschätzer (Stichprobenmittel), welcher wie folgt definiert ist:
	 	\begin{equation*}
	 			\hat{\mu}=\frac{1}{n}\sum_{i=1}^{n} X_i.
	 	\end{equation*}
	 	Hier hängt $\hat{\mu}$ \textbf{nicht} von $\mu_0$ ab!
	 \end{BSP}
	
	

\pagebreak % fuegt einen Zeilenumbruch ein
\subsection{Beispiele von parametrischen Modellen}
	
	\begin{enumerate}[label = (\roman*)]
		\item Bernoulli-Verteilung \\
		Gegeben sind Zufallsvariablen $\FZV$ mit
		\begin{equation*}
			\FZV \overset{\textbf{i.i.d.}}{\sim} B(p_0), \; 0 \leq p_0 \leq 1.
		\end{equation*} 
		Hier ist der wahre Parameter 
		\begin{equation*}
			\theta_0 = p_0
		\end{equation*}
		mit entsprechendem Parameterraum
		\begin{equation*}
			\Theta = [0,1] \subseteq \IR^k,\; k=1.
		\end{equation*}
		
		\item Geometrische Verteilung \\
		Gegeben sind Zufallsvariablen $\FZV$ mit
		\begin{equation*}
			\FZV \overset{\textbf{i.i.d.}}{\sim} G(p_0), \; 0 < p_0 \leq 1.
		\end{equation*} 
		Hier ist der wahre Parameter 
		\begin{equation*}
			\theta_0 = p_0
		\end{equation*}
		mit entsprechendem Parameterraum
		\begin{equation*}
			\Theta = (0,1] \subseteq \IR^k,\; k=1.
		\end{equation*}


	\item Normalverteilung
	\begin{enumerate}[label = (\alph*)] % Alph = Buchstaben
	\item Normalverteilung mit bekannter Varianz $\sigma^2$\\
	Gegeben sind Zufallsvariablen $\FZV$ mit
	\begin{equation*}
		\FZV \overset{\textbf{i.i.d.}} {\sim} N(\mu_0,\sigma^2),\; \mu_0 \in \IR, \; \sigma^2 >0.
	\end{equation*} 
	Hier ist der wahre Parameter 
	\begin{equation*}
		\theta_0 = \mu_0
	\end{equation*}
	mit entsprechendem Parameterraum
	\begin{equation*}
		\Theta = \IR \subseteq \IR^k,\; k=1.
	\end{equation*}
	
	\item Normalverteilung mit bekanntem Erwartungswert $\mu$\\
	Gegeben sind Zufallsvariablen $\FZV$ mit
	\begin{equation*}
		\FZV \overset{\textbf{i.i.d.}} {\sim} N(\mu,\sigma_0^2),\; \mu \in \IR, \; \sigma_0^2 >0.
	\end{equation*} 
	Hier ist der wahre Parameter 
	\begin{equation*}
		\theta_0 = \sigma_0^2
	\end{equation*}
	mit entsprechendem Parameterraum
	\begin{equation*}
		\Theta = (0,\infty) \subseteq \IR^k,\; k=1.
	\end{equation*}
	\item Normalverteilung mit unbekanter Varianz und unbekanntem Erwartungswert\\
	Gegeben sind Zufallsvariablen $\FZV$ mit
	\begin{equation*}
		\FZV \overset{\textbf{i.i.d.}} {\sim} N(\mu_0,\sigma_0^2),\; \mu_0 \in \IR, \; \sigma_0^2 >0.
	\end{equation*} 
	Hier ist der wahre Parameter 
	\begin{equation*}
		\theta_0 = (\mu_0, \sigma_0^2)^\top
	\end{equation*}
	mit entsprechendem Parameterraum
	\begin{equation*}
		\Theta = \IR \times (0,\infty) \subseteq \IR^k,\; k=2.
	\end{equation*}
	\end{enumerate}
\item Gleichverteilung\\
Gegeben sind Zufallsvariablen $\FZV$ mit
\begin{equation*}
	\FZV \overset{\textbf{i.i.d.}}{\sim} U([0,\theta_0]), \; \theta_0 >0.
\end{equation*}
mit entsprechendem Parameterraum
\begin{equation*}
	\Theta = (0,\infty)  \subseteq \IR^k,\; k=1.
\end{equation*}
	\end{enumerate}
	
	
\subsection{Wunschliste für einen Schätzer}
\begin{enumerate}
	\item Unverzerrtheit (Erwartungstreue)\\
	Ein Schätzer enthält keine systematischen Fehler, wenn gilt:
	\begin{equation*}
		\EW_\theta(\hat{\theta}_i) = \theta_i.
	\end{equation*}
	Dies muss für jeden möglichen Wert $\theta \in \Theta$ und für jedes $i = 1, \ldots, k$ gelten. 
	
	\item Konsistenz \\
	Konsistenz bedeutet, dass der Schätzfehler eines Schätzers bei größer werdenden $n$ gegen $0$ geht, also:
	
	
	\begin{equation*}
			\mathbb{P}_\theta(|\hat{\theta}_{ni} - \theta_i| > \epsilon) \overset{n \rightarrow \infty}{\longrightarrow} 0
	\end{equation*}
	Dies muss für jeden möglichen Wert $\theta \in \Theta$ und für jede Komponente $i = 1, \ldots, k$ und für jedes $\epsilon > 0$ gelten.\\
	Konsistenz wird  auch als Konvergenz in Wahrscheinlichkeit bezeichnet:
	\begin{equation*}
		\hat{\theta}_n \overset{p} \longrightarrow \theta \;\; \textbf{für} \;\; n \rightarrow \infty,
	\end{equation*}
	wenn $\theta$ der wahre Parameter ist, \;  $\forall \; \theta \in \Theta$. \\
	
	\pagebreak
	Einschub: Rechenregeln für Konvergenz in Wahrscheinlichkeit\\
	
	
	Es seien $\FZV$ k-dimensionale Vektoren von Zufallsvariablen und $c,d$  feste k-dimensionale Vektoren. 
	\begin{itemize}
		\item Gilt $X_m \KW c$ und ist $f: \IR^k \rightarrow \IR^l$ eine Funktion, die stetig im Punkt $c$ ist, dann gilt auch: \begin{equation*}
		f(X_1) \KW f(c).
		\end{equation*}
		\item Gilt $X_n \KW c$ und $Y_n \KW d$, dann gilt auch:
		\begin{equation*}
			\begin{split}
			X_n \pm Y_n &\KW c \pm d\\
			X_n^\top Y_n &\KW c^\top d.
			\end{split}
		\end{equation*}
	\end{itemize}
	
	\item Genauigkeit bei festem $n$.
	\item Verteilung des Fehlers\\
	$\hat{\theta_n}-\theta_0$ sollte bekannt oder approximierbar sein. 
	
\end{enumerate}

\subsection{Methoden zur Schätzerkonstruktion}
\subsubsection{Momentenmethode}

Man betrachte ein Experiment mit $\FZV$ Zufallsvariablen und trifft die Annahme, dass $\FZV$ i.i.d. Kopien von einer Zufallsvariable $X$ darstellen und einer Verteilung mit bestimmter Form folgen:

\vspace{-2mm}
\begin{equation*}
	X \sim \; \textbf{F}(\theta_0).
\end{equation*}
Diese Verteilung ist abhängig von $\theta_0 \in \Theta \subseteq \IR^k$.

\noindent Jedem $\theta \in \Theta$ entsprechen Momente von $X$, wenn $\theta$ der wahre Parameter ist: 

\begin{equation*}
	\theta = (\theta_1, \ldots, \theta_k)^\top \mapsto (\EW_\theta(X), \EW_\theta(X^2),\ldots,\EW_\theta(X^k))
\end{equation*}

\noindent Die Momentenmethode ist anwendbar, wenn diese Beziehung umkerhbar ist. Das heißt, wenn man aus den gegebenen Momenten die Parameter schätzen kann. Zusätzlich nehmen wir in diesen Kapitel an, dass es eine Funktion $f: \IR^k \rightarrow \IR^k$ gibt, sodass 

\begin{equation*}
	\EW_\theta(X),\ldots,\EW_\theta(X^k) \mapsto f(\EW_\theta(X),\ldots,\EW_\theta(X^k)) \overset{!}= \theta.
\end{equation*}
Dies soll für jedes $\theta \in \Theta$ gelten.

\subsubsection{Beispiele für den Momentenmethoden-Schätzer}

	\begin{enumerate}[label = (\roman*)]
	\item Bernoulli-Verteilung \\
	Gegeben sind Zufallsvariablen $\FZV$ mit
	\begin{equation*}
		\FZV \overset{\textbf{i.i.d.}}{\sim} B(p_0), \; 0 \leq p_0 \leq 1.
	\end{equation*} 
	Hier ist das erste Moment
	\begin{equation*}
		\EW_p(X)=p.
	\end{equation*}
	Somit gilt:
	\begin{equation*}
		f(m)=m.
	\end{equation*}
	
	\item Geometrische Verteilung \\
	Gegeben sind Zufallsvariablen $\FZV$ mit
	\begin{equation*}
		\FZV \overset{\textbf{i.i.d.}}{\sim} G(p_0), \; 0 < p_0 \leq 1.
	\end{equation*} 
	Hier ist das erste Moment
	\begin{equation*}
		\EW_p(X) = \frac{1}{p}.
	\end{equation*}
	Somit gilt:
	\begin{equation*}
		f(m)=\frac{1}{m}.
	\end{equation*}
	
	
		\item Normalverteilung mit unbekanter Varianz und unbekanntem Erwartungswert\\
		Gegeben sind Zufallsvariablen $\FZV$ mit
		\begin{equation*}
			\FZV \overset{\textbf{i.i.d.}} {\sim} N(\mu_0,\sigma_0^2),\; \mu_0 \in \IR, \; \sigma_0^2 >0 \; \textbf{mit} \; \theta_0 = \left(
			\begin{array}{c}
				\mu_0\\
				\sigma_0^2\\
			\end{array}
			\right)
		\end{equation*} 
		Hier ist das erste Moment
		\begin{equation*}
			\EW_{\mu,\sigma^2}(X)=\mu.
		\end{equation*}
		Das zweite Moment enstpricht: 
		\begin{equation*}
			\EW_{\mu,\sigma^2}(X^2)=\textbf{Var}_{\mu,\sigma^2}(X^2) + (\EW_{\mu,\sigma^2}(X))^2 = \sigma^2 + \mu^2
		\end{equation*}
		Also:
		
		\begin{equation*}
			\left(
			\begin{array}{c}
				\mu\\
				\sigma^2\\
			\end{array}
			\right)
			\rightarrow 
			\left(
			\begin{array}{c}
				\mu\\
				\sigma^2+\mu^2\\
			\end{array}
			\right) =
			\left(
			\begin{array}{c}
				m_1\\
				m_2\\
			\end{array}
			\right)
		\end{equation*}
		Hier ist also
		\begin{equation*}
			m_1 = \mu
		\end{equation*}
		und
		\begin{equation*}
				m_2 - m_1^2 = \sigma^2 + \mu^2 - \mu^2 = \sigma^2.
		\end{equation*}
		
		Damit ist hier
		
		\begin{equation*}
			f(\left(
			\begin{array}{c}
				m_1\\
				m_2\\
			\end{array}
			\right)) = 
			\left(
			\begin{array}{c}
				m_1\\
				m_2 - m_1^2\\
			\end{array}
			\right).
		\end{equation*}
		
	

	\item Gleichverteilung\\
	Gegeben sind Zufallsvariablen $\FZV$ mit
	\begin{equation*}
		\FZV \overset{\textbf{i.i.d.}}{\sim} U([0,\theta_0]), \; \theta_0 >0.
	\end{equation*}
	Hier ist das erste Moment
	\begin{equation*}
		\EW_\theta (X) = \frac{\theta} {2} = m \; \textbf{bzw.} \;
		\theta = 2\cdot m = 2 \cdot \EW_\theta(X).
	\end{equation*}
	Somit gilt:
	\begin{equation*}
		f(m) = 2\cdot m.
	\end{equation*}
\end{enumerate}

	\noindent Man betrachte erneut ein Experiment mit $\FZV$ Zufallsvariablen und trifft die Annahme, dass $\FZV$ i.i.d. Kopien von einer Zufallsvariable $X$ darstellen und einer Verteilung mit bestimmter Form folgen:

\vspace{-2mm}
\begin{equation*}
	X \sim \; \textbf{F}(\theta_0).
\end{equation*}

	\noindent Mit den entsprechenden empirischen Momenten kann man die ersten $k$ Momente von $X$ berechnen. 

\begin{equation*}
	\bar{x^j} = \frac{1}{n} \sum_{i=1}^n x^j = \EW_\theta(X^j)\; \textbf{für} \; 1\leq j \leq k 
\end{equation*}

	\noindent Der Momenten-Methoden-Schätzer $\hat{\theta}_{MM}$ für 
\begin{equation*}
\theta_0 = f({\EW_\theta}_{0}(X),\ldots,{\EW_\theta}_{0}(X^k))
\end{equation*}

	\noindent ist der entsprechende Plug-In Schätzer

\begin{equation*}
	\hat{\theta}_{MM} = f(\bar{x},\bar{x^2}, \ldots, \bar{x^k}).
\end{equation*}

\subsubsection{Beispiele für Plug-In Schätzer}
	\begin{enumerate}[label = (\roman*)]
	\item Bernoulli-Verteilung \\
	Gegeben sind Zufallsvariablen $\FZV$ mit
	\begin{equation*}
		\FZV \overset{\textbf{i.i.d.}}{\sim} B(p_0), \; 0 \leq p_0 \leq 1.
	\end{equation*} 
	Hier ist das erste Moment
	\begin{equation*}
		\EW_p(X)=p.
	\end{equation*}
	Also gilt
	\begin{equation*}
		f(m)=m,
	\end{equation*}
	sodass
	\begin{equation*}
		\hat{p}_{MM} = \bar{x} 
	\end{equation*}
	ist.
	
	\item Geometrische Verteilung \\
	Gegeben sind Zufallsvariablen $\FZV$ mit
	\begin{equation*}
		\FZV \overset{\textbf{i.i.d.}}{\sim} G(p_0), \; 0 < p_0 \leq 1.
	\end{equation*} 
	Hier ist das erste Moment
	\begin{equation*}
		\EW_p(X) = \frac{1}{p}.
	\end{equation*}
	Also gilt
	\begin{equation*}
		f(m)=\frac{1}{m},
	\end{equation*}
	sodass
	\begin{equation*}
		\hat{p}_{MM} = \frac{1}{\bar{x}}.
	\end{equation*}
	
	
	\item Normalverteilung mit unbekannter Varianz und unbekanntem Erwartungswert\\
	Gegeben sind Zufallsvariablen $\FZV$ mit
	\begin{equation*}
		\FZV \overset{\textbf{i.i.d.}} {\sim} N(\mu_0,\sigma_0^2),\; \mu_0 \in \IR, \; \sigma_0^2 >0 \; \textbf{mit} \; 	f(\left(
		\begin{array}{c}
			m_1\\
			m_2\\
		\end{array}
		\right)) = 
		\left(
		\begin{array}{c}
			m_1\\
			m_2 - m_1^2\\
		\end{array}
		\right),
	\end{equation*} 

sodass

\begin{equation*}
	\left(
	\begin{array}{c}
		\hat{\mu}_{MM}\\
		\hat{\sigma}^2_{MM}\\
	\end{array}
	\right)) = 
	\left(
	\begin{array}{c}
		\bar{x}\\
		\bar{x^2}-\bar{x}^2\\
	\end{array}
	\right),
\end{equation*}
	
	
	
	\item Gleichverteilung\\
	Gegeben sind Zufallsvariablen $\FZV$ mit
	\begin{equation*}
		\FZV \overset{\textbf{i.i.d.}}{\sim} U([0,\theta_0]), \; \theta_0 >0.
	\end{equation*}
	Hier ist also
\begin{equation*}
	f(m) = 2\cdot m,
\end{equation*}
	sodass
	\begin{equation*}
		\hat{\theta}_{MM} = 2\bar{x}
	\end{equation*}
	
\end{enumerate}

\subsubsection{Eigenschaften der Momentenmethode}

\begin{enumerate}
	\item Erwartungstreue\\
	Es ist bereits bekannt, dass $\EW_\theta(\bar{X^i})=\EW_\theta(X^j)$ ist. Dies gilt für jedes $j \geq 1$:
	
	\begin{equation*}
		\left(
		\begin{array}{c}
			\bar{x}\\
			\bar{x^2}\\
			\vdots\\
			\bar{x^k}\\
		\end{array}
		\right)\; \textbf{ist erwartungstreu für} \;
		\left(
		\begin{array}{c}
		 \EW_\theta(X)\\
		 \EW_\theta(X^2)\\
			\dots\\
		 \EW_\theta(X^k) \\
		\end{array}
		\right).
	\end{equation*}
	 Allerdings ist $\hat{\theta}_{MM} = f(\bar{x}, \ldots, \bar{x}_n)$ nur erwartungstreu für $\theta = f(\EW_\theta(X), \ldots, \EW_\theta(X^k))$, wenn $f(\cdot)$ linear ist. 
	 
	 Ist $f(\cdot)$ nicht linear, dann gilt im Allgemeinen
	 \begin{equation*}
	 	\begin{split}
	 	\EW_\theta(\hat{\theta}_{MM}) &\neq \EW_\theta(f(\bar{X}, \ldots, \bar{X^k}))\\
	 	&\neq f(\EW_\theta(\bar{X}),\ldots, \EW_\theta(\bar{X^k}))\\
	 	&= f(\EW_\theta({X}),\ldots, \EW_\theta({X^k})) =  \theta
	 \end{split}
	 \end{equation*}
	
	Ist f linear, dann gilt Gleichheit für $\EW_{\theta}(f)$ und $f(\EW_{\theta})$, womit $\hat{\theta}_{MM}$ unverzerrt für $\theta$ ist.\\
	
	\textbf{Beispiele Erwartungstreue}

	\begin{enumerate}[label = (\roman*)]
		\item Bernoulli-Verteilung\\
			Gegeben sind Zufallsvariablen $\FZV$ mit
		\begin{equation*}
			\FZV \overset{\textbf{i.i.d.}}{\sim} B(p_0), \; 0 \leq p_0 \leq 1.
		\end{equation*} 
		Hier ist das erste Moment
		\begin{equation*}
			\EW_p(X)=p \; \textbf{und} \; 	f(m)=m,
		\end{equation*}
		sodass
		\begin{equation*}
			\hat{p}_{MM} = \bar{x} 
		\end{equation*}
		linear ist. Daher ist 
		\begin{equation*}
			\EW_p(\hat{p}_{MM}) = \EW_p(\bar{X}) = \EW_p(X) = p
		\end{equation*}
		
			\item Geometrische Verteilung \\
		Gegeben sind Zufallsvariablen $\FZV$ mit
		\begin{equation*}
			\FZV \overset{\textbf{i.i.d.}}{\sim} G(p_0), \; 0 < p_0 \leq 1.
		\end{equation*} 
		Hier ist das erste Moment
		\begin{equation*}
			\EW_p(X) = \frac{1}{p} \; \textbf{und} \;			f(m)=\frac{1}{m},
		\end{equation*}
		sodass
		\begin{equation*}
			\hat{p}_{MM} = \frac{1}{\bar{x}}
		\end{equation*}
		Diese Funktion ist nicht linear. Tatsächlich gilt, dass $\EW_p(\hat{p}_{MM}) = \EW_p (\frac{1}{\bar{x}}) < p, \; \forall  \; p \in (0,1)$. Damit ist der Schätzer verzerrt. Eine Ausnahme bildet $ p = 1 $, in welcher $\hat{p}_{MM}$  unverzerrt ist.
		
		\item Normalverteilung mit unbekannter Varianz und unbekanntem Erwartungswert\\
		Gegeben sind Zufallsvariablen $\FZV$ mit
		\begin{equation*}
			\FZV \overset{\textbf{i.i.d.}} {\sim} N(\mu_0,\sigma_0^2),\; \mu_0 \in \IR, \; \sigma_0^2 >0 \; \textbf{mit} \; 	f(\left(
			\begin{array}{c}
				m_1\\
				m_2\\
			\end{array}
			\right)) = 
			\left(
			\begin{array}{c}
				m_1\\
				m_2 - m_1^2\\
			\end{array}
			\right),
		\end{equation*} 
		
		sodass
		
		\begin{equation*}
			f(\left(
			\begin{array}{c}
				\hat{\mu}_{MM}\\
				\hat{\sigma}^2_{MM}\\
			\end{array}
			\right)) = 
			\left(
			\begin{array}{c}
				\bar{x}\\
				\bar{x^2}-\bar{x}^2\\
			\end{array}
			\right),
		\end{equation*}
		
		$\bar{x}$ ist eine lineare Funktion, $\bar{x^2}-\bar{x}^2$ allerdings nicht.  Daher ist der Schätzer für $\mu$ unverzerrt. 
		
		Die mögliche Erwartungstreue des Schätzers für $\sigma^2$ lässt sich folgendermaßen überprüfen:
		
		\begin{equation*}
			\EW_{\mu,\sigma^2} (\tilde{\sigma}^2_{MM}) = \EW_{\mu,\sigma^2} (\tilde{\sigma}^2) = \frac{n - 1 }{n} \sigma^2 < \sigma^2 
		\end{equation*}
	
			\item Gleichverteilung\\
		Gegeben sind Zufallsvariablen $\FZV$ mit
		\begin{equation*}
			\FZV \overset{\textbf{i.i.d.}}{\sim} U([0,\theta_0]), \; \theta_0 >0 \;\textbf{mit}\; f(m) = 2m,
		\end{equation*}
		sodass
		\begin{equation*}
			\hat{\theta}_{MM} = 2\bar{x}
		\end{equation*}
		Dieser Schätzer bildet eine lineare Funktion. Daher ist der Schätzer erwartungstreu bzw. unverzerrt. 
	\end{enumerate}	
	
	\item Konsistenz \\
	Man betrachte einen Momenten-Methoden-Schätzer $\hat{\theta}_n = f(\bar{X}_n, \bar{X^2}_n, \ldots, \bar{X^k}_n)$. Wie in 1.3.4. gilt mit dem Gesetz der Großen Zahl für jedes $\theta \in \Theta$ und jedes $j \geq 1$ dass $\bar{x^j}_n \longrightarrow \EW_\theta (X^j)$, wenn $\theta$ der wahre Parameter ist. Damit gilt auch, dass
	
		\begin{equation*}
		\left(
		\begin{array}{c}
			\bar{x}_n\\
			\bar{x^2}_n\\
			\dots\\
			\bar{x^k}_n\\
		\end{array}
		\right)\; \overset{p}\longrightarrow \;
		\left(
		\begin{array}{c}
			\EW_\theta(X)\\
			\EW_\theta(X^2)\\
			\dots\\
			\EW_\theta(X^k) \\
		\end{array}
		\right).
	\end{equation*}

	Ist zusätzlich $f: \IR^k \rightarrow \IR^k$ stetig im Punkt $(\EW_\theta(X),\ldots,\EW_\theta(X^k))$, dann folgt auch, dass $\hat{\theta}_n = f(\bar{X_n}, \ldots, \bar{X^k_n}) \overset{p} \rightarrow f(\EW_\theta(X),\ldots,\EW_\theta(X^k)) = \theta$, wenn $\theta$ der wahre Parameter ist. 

	\begin{Definition}[Definition 1.... (Konsistenz von MM-Schätzern)]
	Ein MM-Schätzer $\hat\theta_n = f(\bar{X_n},\ldots,\bar{X^k_n})$, wie in 1.3.4., ist konsistent, wenn die Funktion $f:\IR ^k \rightarrow \IR ^k$ stetig ist in jedem Punkt der Menge $\{(\EW_\theta(X),\ldots, \EW_\theta(X^k)) : \theta \in \Theta\}$ ist.	\end{Definition}

\end{enumerate}

\subsubsection{Verteilung des (skalierten) Fehlers von MM-Schätzern}

	Im Gauß'schen Modell ist die Verteilung des Fehlers bekannt: \\
	Man  betrachte $\FZV \overset{\textbf{i.i.d.}} {\sim} N(\mu_0,\sigma_0^2),\; \mu_0 \in \IR, \; \sigma_0^2 >0$, sowie die MM-Schätzer:
	
	\begin{equation*}
	\begin{split}
		\hat{\mu}_n =& \frac{1}{n}\sum_{i=1}^{n} X_i\\
		\tilde{\sigma^2_n} =&\frac{1}{n}\sum_{i=1}^{n} (X_i - \bar{X}_n)^2\\
		\hat{\sigma^2_n}=&\frac{1}{n-1}\sum_{i=1}^{n} (X_i - \bar{X}_n)^2.
	\end{split}
	\end{equation*}
	
		\noindent $\textbf{Schätzung von} \; \mu_0$
	
		\noindent Hier  erhält man ein Kofindenzintervall oder Tests für Hypothesen über $\mu_0$, welche eine exakte Verteilung beinhalten und somit exakt sind:
	
	\begin{equation*}
		\sqrt{n} \; \frac{\hat{\mu}_0 - \mu}{\hat{\sigma}} \;{\sim}\; t_{n-1}
	\end{equation*}
	
		\noindent Der Ausdruck ist berechenbar und die Verteilung ist bekannt.\\

		\noindent $\textbf{Schätzung von} \; \sigma^2_0$\\
	Hier erhält man ein Kofindenzintervall für $\sigma^2_0$ oder Tests für Hypothesen über $\sigma^2_0$, welche ebenso exakt sind:
	
	
	\begin{equation*}
		(n-1) \; \frac{\hat{\sigma^2}_0}{\sigma^2_0} \; {\sim} \; \chi^2_{n-1}
	\end{equation*}
	
	
		\noindent Ist der MM-Schätzer durch eine $\textbf{lineare Funktion}$ definiert, dann kann man den zentralen Grenzwertsatz und die Rechenregeln aus der Vorlesung Grundzüge der Statistik Kapitel 8.3. verwenden. 
	
	\noindent Wenn gilt
	
	\begin{equation*}
		\begin{split}
				X_n \KV& X \\
			Y_n \KW& c 
		\end{split}
	\end{equation*}
	
	\noindent dann gilt auch
	
	\begin{equation*}
		Y_n \cdot X_n \KV X\cdot c
	\end{equation*}
	
	
	\subsubsection{Beispiele für die Verteilung des (skalierten Schätzfehlers)}
	
	\begin{enumerate}[label = (\roman*)]
		\item Bernoulli-Verteilung\\
		Gegeben sind Zufallsvariablen $\FZV$ mit
		\begin{equation*}
			\FZV \overset{\textbf{i.i.d.}}{\sim} B(p_0), \; 0 \leq p_0 \leq 1.
		\end{equation*} 
		Hier ist
		\begin{equation*}
			\hat{p}_{MM} = \bar{x},
		\end{equation*}
		und der Erwartungswert und die Varianz:
		\begin{equation*}
			\EW_p(X)=p \; \textbf{und} \; 	Var_p(X) = p(p-1)
		\end{equation*}
		Mit dem Zentralen Grenzwertsatz gilt:
		\begin{equation*}
			\sqrt{n}(\hat{p}_n - p_0) \KV N(0,p_0(1-p_0))
		\end{equation*}
		
		Leider ist der Parameter $p_0$ unbekannt. Allerdings gilt:
		
		\begin{equation*}
			\hat{p}_n \KW p_0
		\end{equation*}
		
		Damit gilt auch:
		
		\begin{equation*}
			\begin{split}
				\hat{p}_n (1-\hat{p}_n) &\KW p_0(1-p_0) \\
				\sqrt{\hat{p}_n (1-\hat{p}_n)} &\KW \sqrt{p_0(1-p_0)} \\
				\frac{1}{\sqrt{\hat{p}_n (1-\hat{p}_n)}} &\KW \frac{1}{\sqrt{p_0(1-p_0)}}
			\end{split}
		\end{equation*}
		
		Somit gilt mit dem Zentralen Grenzwertsatz also:
		
		\begin{equation*}
			\frac{\sqrt{n} (\hat{p}_n - p_0)}{\sqrt{\hat{p}_n (1-\hat{p}_n)}} \sim N(0,1)
		\end{equation*}
		
		Dieser Term ergibt sich aus folgender Überlegung:
		
		\begin{equation*}
			\frac{\sqrt{n} (\hat{p}_n - p_0)}{\sqrt{p_0 (1-p_0)}} \cdot \frac{\sqrt{p_0(1-p_0)}}{\sqrt{\hat{p}_n (1-\hat{p}_n)}}
		\end{equation*}
		
		Der erste Faktor konvergiert in Verteilung gegen eine Standardnormalverteilung und der zweite Faktor konvergiert in Wahrscheinlichkeit gegen 1 aufgrund der zuvorigen Konvergenz in Wahrscheinschleichkeit. 
		Wegen den Rechenregeln der Konvergenz, konvergiert der gesamte Term, wie oben beschrieben gegen eine Standardnormalverteilung. 
		Solche Prozeduren nennt man $\textbf{asymptotisch valide}$
		
		
		
		
		\item Gleichverteilung\\
		Gegeben sind Zufallsvariablen $\FZV$ mit
		\begin{equation*}
			\FZV \overset{\textbf{i.i.d.}}{\sim} U([0,\theta_0]), \; \theta_0 >0 \;\textbf{mit}\; \hat{\Theta}_{MM} = 2\bar{x}
		\end{equation*}
		Der Erwartungswert und die Varianz sind: 
		\begin{equation*}
			\EW_\theta (X) = \frac{\theta}{2} \; \textbf{und} \; Var_\theta (X) = \frac{\theta^2}{12}
		\end{equation*}
		Mit dem zentralen Grenzwertsatz gilt:
		
		\begin{equation*}
			\sqrt{n} \left(\bar{X}_n - \frac{\theta_0}{2}\right) \KV N\left(0, \frac{\theta^2_0}{12}\right)
		\end{equation*}
		
		ähnlich wie im Bernoulli-Modell gilt
		
		\begin{equation*}
			\frac{\sqrt{n}(\hat{\theta}_n - \theta_0)}{\sqrt{\frac{\theta^2_n}{3}}} \; = \; 
			\frac{\sqrt{n}(2 \bar{X}_n - \theta_0)}{\sqrt{\frac{4\bar{X}^2_n}{3}}} \; = \; \frac{\sqrt{n}(\bar{X}_n - \frac{\theta_0}{2})}{\sqrt{\frac{\bar{X}^2_n}{3}}}	
		\end{equation*}
		
		Der Zähler konvergiert in Verteilung gegen eine Normalverteilung mit Erwartungswert 0 und einer Varianz $\frac{\theta^2_0}{12}$. Der Nenner konvergiert in Wahrscheinlichkeit gegen $\sqrt{\frac{\theta^2}{12}}$. 
		
	\end{enumerate}	
	
	
	\subsubsection{Delta-Methode}
	\noindent In vielen Fällen ist der MM-Schätzer definiert durch eine nicht-lineare Funktion. Hier hilft die sogenannte $\delta \textbf{-Methode}$. Der Einfachheithalber sei $k = 1$.
	
	\noindent Es seien $Z_1, \ldots, Z_n$ i.i.d. Kopien von $Z$ mit $\EW(Z)=v$ und $Var(Z)=\delta^2 > 0$. Weiters sei $f: \IR \rightarrow \IR$ differenzierbar im Punkt $v$. Dann gilt:
	
	\begin{equation*}
		\sqrt{n} (f(\bar{Z}_n)-f(v)) \sim N(0, \delta^2 (f'(v))^2)
	\end{equation*}
	
	\noindent Wobei $\bar{Z}_n =  \frac{1}{n}\sum_{i=1}^{n} Z_i$.
	
	\begin{BSP}[Beispiel 1..... (Delta-Methode)]
		
		Geometrische Verteilung \\
		Gegeben sind Zufallsvariablen $\FZV$ mit
		\begin{equation*}
			\FZV \overset{\textbf{i.i.d.}}{\sim} G(p_0), \; 0 < p_0 \leq 1.
		\end{equation*} 
		Der Erwartungswert und die Varianz sind:
		\begin{equation*}
			\EW_p(X) = \frac{1}{p} \; \textbf{und} \; Var_p(X) = \frac{1-p}{p^2}
		\end{equation*}
		sodass
		\begin{equation*}
			\hat{p}_{MM} = \frac{1}{\bar{x}_n} = f(\bar{x}_n) \; \textbf{für} \; f(t)= \frac{1}{t}
		\end{equation*} 
		Die Tangente $f(t) = \frac{1}{t}$ an der Stelle $t^* = \frac{1}{p} = \EW_p(X)$
		
		\begin{equation*}
			f\left(\frac{1}{p}\right) + f'\left(\frac{1}{p}\right) \cdot \left(t-\frac{1}{p}\right)\\
			= p + f'\left(\frac{1}{p}\right) \left(t-\frac{1}{p}\right)
		\end{equation*}
		Für $t=\bar{X}_n$ gibt das
		
		\begin{equation*}
			p - f'\left(\frac{1}{p}\right) \left(\bar{x}_n - \frac{1}{p}\right) \\
			= p - f'\left(\frac{1}{p}\right) (\bar{x}_n - \EW_p(x))
		\end{equation*}
		
	\end{BSP}
	
	
	\begin{BWS}[Beweis 1.1 Delta-Methode]
		Es gilt:
		
		
		\begin{equation*}
			\bar{Z}_n \KW v  \eqname{(1)}
		\end{equation*}
		
		Hier gilt das Gesetz der großen Zahl
		
		\begin{equation*}
			\sqrt{n} (\bar{Z}_n - v) \KV N(0,\delta^2) \eqname{(2)}
		\end{equation*}
		Hier gilt der Zentrale Grenzsatz\\
		
		$f$ ist differenzierbar an der Stelle v, das heißt: 
		
		\begin{equation*}
			f'(v) = \underset{n \rightarrow v}{lim} \frac{f(z) - f(v)}{z-v} \eqname{(3)}
		\end{equation*}
		
		Definiere eine Funktion $R(z)$ als
		
		\begin{equation*}
			R(z) = \begin{cases} 
				f'(v) - \frac{f(z)- f(v)}{z-v} &\text{wenn } z \neq v\\
				0 &\text{wenn } z = v
			\end{cases} \eqname{(4)}
		\end{equation*}
		
		Hierbei muss beachtet werden das $R(z)$ ist an der Stelle $z=v$, weil $\underset{n \rightarrow v} {lim} R(z) = R(v)$
		
		\begin{equation*}
			\underset{n \rightarrow v} {lim} R(z) = 0 = R(v)
		\end{equation*}
		
		Dies gilt wegen (3) und (4).
		
		Aufgrund von (4) gilt auch für $z \neq v$:
		
		\begin{equation*}
			f'(v)=\frac{f(z) - f(v)}{z - v} + R(z)
		\end{equation*}
		
		Mit einer Multiplikation mit $z-v$ ergibt sich für $z \neq v$ aber offensichtlich auch für $z = v$:
		
		\begin{equation*}
			f'(v) \cdot (z-v) = f(z) - f(v) + R(z) \cdot (z-v)
		\end{equation*}
		
		Wenn man nun $z$ durch $\bar{z}_n$ ersetzt, ergibt dies:
		
		
		\begin{equation*}
			f'(v) \cdot (\bar{z}_n-v) = f(\bar{z}_n) - f(v) + R(\bar{z}_n) \cdot (\bar{z}_n-v)			
		\end{equation*}
		
		Wenn nun weiters mit $\sqrt{n}$ multipliziert wird gilt:
		
		\begin{equation*}
			f'(v) \cdot\sqrt{n} (\bar{z}_n-v) = \sqrt{n}(f(\bar{z}_n) - f(v)) + \sqrt{n} (\bar{z}_n-v)	R(\bar{z}_n)
		\end{equation*}
		
		Hierbei gelten folgende Konvergenzsätze:
		
		\begin{equation*}
			\begin{split}
			R(\bar{z}_n) \KW& \;0\\
			\sqrt{n} (\bar{z}_n-v) \KV& \;N(0,\delta^2)\\
			f'(v) \cdot\sqrt{n} (\bar{z}_n-v) \KV& \;N(0,\delta^2 \cdot (f'(v))^2)
			\end{split}
		\end{equation*}
		
		
	\end{BWS}
	
	\noindent Die Delta-Methode ist anwendbar auf MM-Schätzer deren Form $\hat{\theta}_n = f(\bar{X}_n)$, wenn die Funktion $f$ differenzierbar ist in jeden Punkt der Menge ${\EW_\theta(X) : \theta \in \Theta}$
	
	
	
	\begin{BSP}[Beispiel 1..... (Delta-Methode)]		
		Geometrische Verteilung \\
		Gegeben sind Zufallsvariablen $\FZV$ mit
		\begin{equation*}
			\FZV \overset{\textbf{i.i.d.}}{\sim} G(p_0), \; 0 < p_0 \leq 1.
		\end{equation*} 
		Der Erwartungswert und die Varianz sind:
		\begin{equation*}
			\EW_p(X) = \frac{1}{p} \; \textbf{und} \; Var_p(X) = \frac{1-p}{p^2}
		\end{equation*}
		
		Hier ist
		\begin{equation*}
			\hat{p}_n = \frac{1}{\bar{x}_n}, \EW_p(X) = \frac{1}{p}, Var(X) = \frac{1-p}{p^2} 
		\end{equation*}
		
		Es ist
		\begin{equation*}
			{\EW_p(x) : 0<p<1} = {\frac{1}{p}: 0<p<1} = [1, \infty)
		\end{equation*}
		
		Die Funktion $f(m)= \frac{1}{m}$ ist differenzierbar in jedem Punkt dieser Menge mit $f'(m)= - \frac{1}{m^2}$. 
		
		Mit dem Zentralen Grenzwertsatz gilt:
		
		\begin{equation*}
			\sqrt{n}\left(\bar{x}_n - \frac{1}{p}\right) \KV N\left(0, \frac{1-p}{p^2}\right),
		\end{equation*}
		
		wenn $p$ der wahre Parameter ist. 
		
		Mit der Delta-Methode gilt:
		
		\begin{equation*}
			\begin{split}
				\sqrt{n} \left(f\left(\bar{X}_n-f\left(\frac{1}{p}\right)\right)\right) \KV&\; N\left(0,\frac{1-p}{p^2}\left(f'\left(\frac{1}{p}\right)\right)^2\right) \\
				N\left(0,\frac{1-p}{p^2}\left(f'\left(\frac{1}{p}\right)\right)^2\right) = &\;N(0,p^2(1-p))
			\end{split}
		\end{equation*}
		
		wenn $p$ der wahre Parameter ist.
		
		$\hat{p}_n$ ist konsistent für $p$, das heißt $\hat{p}_n \KW p$.
		Die Funktion $\frac{1}{\sqrt{\hat{p}_n^2(1-\hat{p}_n)}}$ ist stetig in $p$ für $0<p<1$, sodass  $\frac{1}{\sqrt{\hat{p}_n^2(1-\hat{p}_n)}} \KW  \frac{1}{\sqrt{p^2(1-p)}}$.
		
		Konvergenz in Wahrscheinlichkeit bleibt bei Stetigkeit erhalten. Damit gilt:
		
		\begin{equation*}
			\frac{\sqrt{n}(\hat{p}_n-p)}{\sqrt{\hat{p}_n^2(1-\hat{p}_n)}} \KV N(0,1)
		\end{equation*}
		
		Damit kann man asymptotisch valide Tests oder Konfidenzintervalle für $p$ konstruieren. Beispielsweise ein Kondfidenzintervall für $p$ mit Überdeckungswahrscheinlichkeit $1-\alpha$ ist gegeben durch:
		
		\begin{equation*}
			\hat{p}_n \pm \sqrt{\frac{\hat{p}_n^2 (1-\hat{p}_n)}{n}} \phi^{-1} \left(1-\frac{\alpha}{2}\right)
		\end{equation*}
		
	\end{BSP}
	
	\noindent Die Überdeckungswahrscheinlichkeit des Kondfidenzintervalls im letzten Beispiel ist nur nominal gleich $1-\alpha$ und hängt ab von $n$ und $p$. 
	
	\noindent Wenn das wahre $p$ bei $0,5$ liegt reichen nahezu $400$ Stichproben. 
	
	\noindent Die tatsächliche Überdeckungswahrscheinlichkeit konvergiert für  $n \rightarrow \infty$ gegen $1-\alpha$. Der Fehler (Überdeckungs-wahrscheinlichkeit - $(1-\alpha)$) hängt von $n$ und $p$ ab. Das heißt die Konvergenz hier ist nicht gleichmäßig in $p$. Zum Glück kann man $p$ konsistent schätzen. Dieses und ähnliche Phänomene treten bei approximierten Verfahren häufig auf. 
	
	
	\subsubsection{Maximum-Likelihood-Methode (ML)}
	
	Man betrachte ein Modell wie in 1.: $\FZV$ sind i.i.d. Kopien der Zufallsvariable $X$ deren Verteilung eine bestimmte Form ist. 
	
	\begin{equation*}
		X \sim \; \textbf{F}(\theta_0) \;	\theta \in \Theta \subseteq \IR^k
	\end{equation*}
	 
	\noindent Im sogenannten diskreten Modell ist $X$ diskret und wird druch eine Wahrscheinlichkeitsfunktion $p(x \mid \theta_0)$ beschrieben. 
	Im sogenannten stetigen Modell ist $X$ stetig und wird druch eine Dichtefunktion $f(x \mid \theta_0)$ beschrieben. 
	
	\begin{Definition} [Definition 1.... Likelihodd-Funktion]
		Die Likelihood-Funktion wird definiert als
		\begin{equation*}
			L(\theta) = \prod_{i=1}^{n} f(x_i \mid \theta) \eqname{stetig}
		\end{equation*}
		\begin{equation*}
			L(\theta) = \prod_{i=1}^{n} p(x_i \mid \theta) \eqname{diskret}
		\end{equation*}
		für $\theta \in \Theta$.
	\end{Definition}
	
	Oft betrachtet man auch die Log-Likelihood
	
	\begin{Definition} [Definition 1.... Log-Likelihood-Funktion]
		\begin{equation*}
			l(\theta) = \log ( L(\theta)) = \begin{cases}
				\sum_{i=1}^{n} \log (f(x_i \mid \theta)) \\
				\sum_{i=1}^{n} \log (p(x_i \mid \theta)) 
			\end{cases}
		\end{equation*}
	\end{Definition}
	
	Beachte: 
	
	\begin{equation*}
		\begin{split}
			L(\theta) =& L(\theta, \FZV)\\
			l(\theta) =&l(\theta, \FZV)
		\end{split}
		\eqname{zufällig}
	\end{equation*}
	
	\subsubsection{Beispiele Maximum-Likelihood-Schätzer}
		\begin{enumerate}[label = (\roman*)]
		\item Bernoulli-Verteilung \\
		Gegeben sind Zufallsvariablen $\FZV$ mit
		\begin{equation*}
			\FZV \overset{\textbf{i.i.d.}}{\sim} B(\theta_0), \; 0 \leq \theta_0 \leq 1.
		\end{equation*} 
		Für $x \in \{0,1\}$
		\begin{equation*}
			p(\xt) = \begin{cases}
				\theta &: x=1 \\
				1- \theta&: x=0
			\end{cases} = \theta^x (1-\theta)^{1-x}
		\end{equation*}
		Daraus ergibt sich die Maximum-Likelihood-Funktion
		\begin{equation*}
			L(\theta) = \prod_{i=1}^{n} p(x_i \mid \theta) =\prod_{i=1}^{n} \theta^{x_i} (1-\theta)^{x_i} = \theta^{\sum_{i=1}^{n}x_1}(1-\theta)^{\sum_{i=1}^{n}x_1}
		\end{equation*}
		Daraus ergibt sich die Log-Liklihood-Funktion
		\begin{equation*}
			l(\theta) = \left(\sum_{i=1}^{n}x_1\right) \log \theta + \left(n-\sum_{i=1}^{n}x_1\right) \log(1-\theta) = n\bar{x}\log\theta + (n-n\bar{x})\log(1-\theta)
		\end{equation*}
		ist.
		
		\item Geometrische Verteilung \\
		Gegeben sind Zufallsvariablen $\FZV$ mit
		\begin{equation*}
			\FZV \overset{\textbf{i.i.d.}}{\sim} G(\theta_0), \; 0 < \theta_0 < 1.
		\end{equation*} 
		Hier ist für $x\in \mathbb{N}$
		\begin{equation*}
			p(\xt)=(1-\theta)^{x-1}\theta
		\end{equation*}
		Daraus ergibt sich die Maximum-Likelihood-Funktion
		\begin{equation*}
			L (\theta) = \prod_{i=1}^{n} ((1-\theta)^{x-1}\theta) = (1-\theta)^{\sum_{i=1}^{n}x_i -n} \theta^n = (1-\theta)^nx-n \theta^n
		\end{equation*}
		Daraus ergibt sich die Log-Likelihood-Funktion
		\begin{equation*}
			\log(\theta)= (\sum_{i=1}^{n}x_i -n)\log(1-\theta)+n\log(\theta) = (n\bar{x}-n)\log(1-\theta)+ n\log\theta
		\end{equation*}
		
		
		\item Normalverteilung mit unbekannter Varianz und unbekanntem Erwartungswert\\
		Gegeben sind Zufallsvariablen $\FZV$ mit
		\begin{equation*}
			\FZV \overset{\textbf{i.i.d.}} {\sim} N(\mu_0,\sigma_0^2),\; \mu_0 \in \IR, \; \sigma_0^2 >0 \; 
		\end{equation*} 
		Für $x \in \IR$		
		\begin{equation*}
			f(x \mid \mu, \sigma^2) = \phi_{\mu,\sigma^2}(x)=(2\pi\sigma^2)^{-\frac{1}{2}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}
		\end{equation*}
		
		Daraus ergibt sich die Maximum-Likelihood-Funktion
		\begin{equation*}
			L(\mu, \sigma^2) = \prod_{i=1}^{n}((2\pi\sigma^2)^{-\frac{1}{2}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}) = (2\pi\sigma^2)^{-\frac{n}{2}}e^{-\frac{n}{2\sigma^2}\sum_{i=1}^{n}(x_i-\mu)^2}
		\end{equation*}
		
		Daraus ergibt sich die Log-Likelihood-Funktion
		
		\begin{equation*}
			l(\mu, \sigma^2) = -\frac{n}{n}\log(2\pi) - \frac{n}{2}\log(\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(x_i-\mu)^2
		\end{equation*}
		
		\item Gleichverteilung\\
		Gegeben sind Zufallsvariablen $\FZV$ mit
		\begin{equation*}
			\FZV \overset{\textbf{i.i.d.}}{\sim} U([0,\theta_0]), \; \theta_0 >0.
		\end{equation*}
		Hier ist für x $\geq 0$
		\begin{equation*}
			f(\xt)=\begin{cases}
				\frac{1}{\theta} &: x\leq \theta\\
				0 &: x >\theta
			\end{cases}
		\end{equation*}
		Daraus ergibt sich die Maximum-Likelihood-Funktion
		\begin{equation*}
			L(\theta)=\prod_{i=1}^{n} f(x_i \mid \theta) = \begin{cases}
				\frac{1}{\theta^n} &: x_i\leq \theta\\
				0 &: \textbf{sonst}
			\end{cases} = \frac{1}{\theta^n} \mathbb{I}\{X_{(n)} \leq \theta\}
		\end{equation*}
		
	\end{enumerate}	
\end{document}
